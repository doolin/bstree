\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fancybox}
\usepackage{tikz}
\usepackage{comment}


\usepackage{cite}

\usepackage{listings}
\usepackage{color}

%%% begin for sentence numbering
%\usepackage{babel, scrjura, xcolor, etoolbox}
%\usepackage{scrjura, xcolor, etoolbox}
% \usepackage{scrjura}

%\useshorthands{/}
%\defineshorthand{/S}{\Sentence\ignorespaces}
%\defineshorthand{/.}{. \Sentence\ignorespaces}

%\makeatletter
%\preto\contract@paragraph@font{\color{white}}
%\makeatother
%%% ^^^ for sentence numbering

\usepackage{hyperref}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset{ %
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  % deletekeywords={...},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,                    % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{blue},       % keyword style
  language=Octave,                 % the language of the code
  % otherkeywords={*,...},           % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},     % string literal style
  tabsize=2,                   % sets default   tabsize to 2 spaces
  title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}

% \language=[Objective]{Caml}


\title{Some notes on binary search trees}
\date{\today}
\author{David M. Doolin}


\begin{document}

\maketitle

\abstract{A few notes on binary search trees and their implementations
in various programming languages.}

\tableofcontents

\section{Introduction}


The following topics will be discussed:

\begin{itemize}

\item Differences in implementations between programming languages.
\item How best to containerize, including pure tree node, tree wrapper with
node class, abstracting keys.
\item Theory and performance.

\end{itemize}

\newcounter{sno}
\setcounter{sno}{0}
\newcommand\sno{\stepcounter{sno}$^{\thesno}$}


\section{Literature review}

Skiena~\cite[pp. 77, 370, 375, 589]{skiena} has a few notes.

Aho and Ullman~\cite[pp. 210]{aho:av:1992} define height and depth for nodes in a binary tree.

\begin{quote}
The height of a node n is the length of a longest path from n to
a leaf. The height of the tree is the height of the root. The depth or level of
a node n is the length of the path from the root to n.
\end{quote}

Rosen~\cite{rosen}...

Cormen et al.~\cite{cormen:th:1990} remains a classic reference.

Rosen~\cite[pp. 757-760]{rosen} discusses three uses for binary search trees:

\begin{enumerate}
\item storing items from a list such that they can be easily found;
\item finding an object in a collection of similar objects;
\item efficiently encoding characters in a bit string.
\end{enumerate}

Sedgewick~\cite{sedgewick:r1990}.

O'Rourke~\cite{orourke:j1998}.

Manber~\cite[p. 87, Ex. 4.8]{manber:u1989} provides an interesting exercise:
show the AVL tree formed by inserting the ordered sequence $[1, 2, \ldots, 20]$.

\section{The Binary Search Tree}

Two main ideas:

\begin{enumerate}
\item Operations for manipulating BSTs;
\item intrinsic properties of BSTs.
\end{enumerate}

Operations include:

\begin{itemize}
\item insert node
\item depth-first search
\item breadth-first search
\item node presence
\item delete node
\item node successor and predecessor
\end{itemize}

Properties include:

\begin{itemize}
\item height
\item depth
\item maximum key
\item minimum key
\item size
\item diameter
\item class (full, perfect, complete, balanced, etc.)
\end{itemize}


\subsection{Operating on the BST}


\subsubsection{Successor and predecessor}

\setcounter{sno}{0}

\sno For an ordered list of values $x_i \in [x_0, x_1,\ldots,x_k]$, the successor of
$x_i$ is $x_{i+1}$, the predecessor $x_{i-1}$.

\sno Cormen et al.~\cite[p. 248-249]{cormen:th:1990} provide a succinct algorithm for
determining the successor given each node is linked to its parent.

\sno A decision has to made regarding the definition of minimum and maximum
elements in the tree. \sno The choices are having the relevant successor or
predecessors of such elements either be nil or be the node. \sno For this implementation,
maximum nodes are their own successors and minimum nodes are their
own predecessors.

\sno Here's an implementation in Python. \sno The nodes do not have a parent
pointer, so the parent must be passed along the recursion, along with
the correct predecessor.

\lstset{language={Python}}
\begin{lstlisting}[frame=single]
def get_predecessor(self, node, parent, predecessor):
    if parent.right == self:
        predecessor = parent

    if node.key == self.key:
        if self.left is not None:
            return self.left.minimum()
    else:
        return predecessor

    if node.key < self.key:
        if self.left is not None:
            return self.left.get_predecessor(node, self, predecessor)
    else:
        if self.right is not None:
            return self.right.get_predecessor(node, self, predecessor)

def predecessor(self, node):
    return get_predecessor(node, self, node)
\end{lstlisting}

\sno Implementing {\tt successor} is easy, use right for left, and find the
maximum where predecessor finds the minimum.



\subsection{Properties of BST}


\subsubsection{Maximum and minimum}

\setcounter{sno}{0}

\sno Maximum and minimum elements of binary search trees are probably the easiest
algorithms to both understand and implement. \sno The maximum is the right-most
node, the minimum the left-most, finding either means traversing the tree
to the appropriate leaf, and returning that node which has no children.

\sno The maximum function written in Ruby is particularly elegant,
we take advantage of Ruby 2.3.x's ``safe navigation'' operator {\tt \&}:
\lstset{language={Ruby}}
\begin{lstlisting}[frame=single]
def maximum:
  self&.maximum || self
end
\end{lstlisting}

\subsection{Collisions or duplicate values}

None the preceding references explicitly discuss collisions.

\begin{itemize}
\item The duplicate key can be added to the tree, either silently or with a warning.
\item The duplicate key is not added to the tree, either silently or with a warning.

\item In Ruby, does replacing a node in a tree leak memory, or will the deleted node
get garbage collected? How about Python?

\item How to delete in c/c++ such that memory isn't leaked? This should be easier,
call delete after the node is orphaned.
\end{itemize}

\section{Persistence}

Here are some ways to store the structure in text format:

\begin{enumerate}
\item Write to nested json. This requires writing out from the top
down. Should be able to do this by converting to hash, then
writing to json. Reading in is the reverse. How to convert a
hash to a binary tree? Maybe this is where to use combinator.

\item CSV: does this require top down, or can it be done by traversing?
Does it require a parent pointer?

\item Yaml: how are references handled?
\end{enumerate}


\section{Breadth-first traversal}

Breadth-first traveral can be used for searching, for determining whether a key exists,
or for writing out the tree in flat format with rows and pointers.

For persistence, we have the following:

\begin{itemize}
\item At each node we can store pointers to each child.
\item We can store the start and the stop of each row separately.
\item We can store an array of arrays, where each of the arrays is
      one level of the tree.
\end{itemize}

[
 [root],
 [ root.left, root.right],
 [...]
]

Can all the pointers be stored in one pass, then process in a second pass,
or can the nodes be processes as the breadth-first traverse proceeds?



\section{Containerizing}

\subsection{Tree/Node interaction}

Trees are abstract data types. This makes trees easy to reason about.
Implementing a tree requires dealing with a number of subtle issues.
Since Trees are usually described in terms of the Nodes comprising
the Tree, some thought about differentiating Trees and Nodes is in order.

\subsubsection{Implicit structure}

Given some class, record, structure or whatever language feature which contains
data and supports function invocation, adding left and right pointers,
appropriate comparators, and insert and find methods suffices to provide
dictionary capability to the given language feature. However, trees require a
root, and managing this root increases the complexity whatever application
system requires the data sorted into the tree. Provision needs to be made for
the root changing, or even being deleted.

\subsubsection{Tree container}

Another way to implement trees is to create a tree container which implements
insert, search, etc. The implementation provided by the tree container could
implement the functionality itself, or call an implementation provided by
the node element.

\subsubsection{The BST Node}

By definition, each node in a binary search tree contains a left child and
a right child, either or both of which may be nil.

Some operations on binary search trees, such as {\tt delete}, require
the node's parent as well as its children. The parent to any node can also
be stored as a reference or pointer, or the parent can be determined at
runtime.

Storing parent pointers is easy. When inserting a node as a child, set that
node's parent pointer to the inserting node.

Determining parent pointers at runtime is not conceptually much more difficult:
keep track of the parent at each node by passing the current node to the
recursive calls. Two disadvantages of this include longer argument lists
for recursive calls, and either returning both node and parent when necessary.
For languages which do not allow multiple return values, one of the arguments
will have to capture the state change for access by the calling method.
In theory, this is straightforward. From an engineering standpoint, requiring
both a return value and an argument state change violates the principle of
least astonishment.

Passing both node and parent back through the argument list is a pretty good
case for adding the parent node directly to the Node data structure.


\subsection{C++ templating}

\subsection{C struct inclusion}

\paragraph{private headers}

\subsection{Ruby module inclusion}

It's not difficult to encode binary search tree capability into a Ruby module.
The module provides all the necessary instance binary search tree methods, and
raises errors when necessary overriding has not been implemented in the
including class.

The usual caveats about instance variables referenced in modules applies,
particularly that the including class must define the key for the tree.

\section{Summary}


\subsection{Links}

\begin{itemize}
\item \href{http://aofa.cs.princeton.edu/60trees/}{http://aofa.cs.princeton.edu/60trees/}
\item \href{https://en.m.wikipedia.org/wiki/Stern-Brocot\_tree}{https://en.m.wikipedia.org/wiki/Stern-Brocot\_tree}
\item \href{https://en.m.wikipedia.org/wiki/Day-Stout-Warren\_algorithm}{%
https://en.m.wikipedia.org/wiki/Day-Stout-Warren\_algorithm}
\item \href{https://en.m.wikipedia.org/wiki/Tango\_tree}{https://en.m.wikipedia.org/wiki/Tango\_tree}
\item \href{https://en.wikipedia.org/wiki/Geometry\_of\_binary\_search\_trees}{%
https://en.wikipedia.org/wiki/Geometry\_of\_binary\_search\_trees}
\end{itemize}


% \section{References}

\bibliography{references}{}
\bibliographystyle{plain}


\appendix

\section{Implementation}

Implementing binary search trees is superficially very easy. Nevertheless,
a number of design decisions must be made. There is also the challenge of
implementing any algorithm well.

One of the gaols of this exercise is to implement binary search trees in multiple
programming languages, including languages with which the author is either
minimally competent, or unfamiliar. Expertise is acquired by hard work and
learning from mistakes, and the faster those mistakes are exposed, the
faster expertise may be approached.

In general, the various implementations were developed following this
pattern:

\begin{enumerate}
\item Install a minimal development environment for each language, focusing on
application structure where testing is supported. Languages without a workable
testing framework have not (and likely will not) be used.
\item Implement a simple, working binary search tree using test driven development.
The initial implementation will be massively over-tested, a consequence of
test-driving each method's implementation.
\item The test suite now provides regression protection, allowing simplistic
method implementations to be iteratively refined, with the goal being production-quality
code indistinguishable from professionally and idiomatically implemented code
for each programming language. This refinement may take more than a single iteration.
\end{enumerate}

For example, the {\tt is\_bst} function implemented in Java used a helper
method on the first iteration. A better way to implement might be to pass
a closure to a generic {\tt in\_order\_traverse} function, similar to how
the Ruby {\tt bst?} implementation works.

\subsection{Handling null/NULL/nil/etc}

Key to any BST algorithm is the notion that leaf nodes are represented
by empty leaf nodes. Empty nodes have no key, no value, and need not be
instantiated. They must, however, be denoted in some way, which is
usually accomplished by setting the relevant child pointers (left and
right) to whatever the programming language regards as a ``non-value.''
For example, in C this is {\tt NULL}, in C++ {\tt nullptr}, in Python {\tt None}.

(It would be possible to define a global external node.)

\subsection{Testing}

Two ways of arranging tests include:
\begin{enumerate}
\item Group tests by method or function, ensuring a number of different
trees pass the method being tested.
\item Group tests by the tree, ensuring each operation on that particular tree
executes correctly.
\end{enumerate}

Traditionally, in Test-Driven Design (TDD), a method test is created with a simple example.
The method is then implemented as simply as possible to ensure the example passes.
More complex examples are created, and the method's implementation modified as necessary
to ensure tests pass. For example, when implementing {\tt insert}, it makes sense
to write a test specification to check the a node is correctly inserted into an
empty tree to form the root, another test specification to ensure the inserting
to the left of root works correctly, to the right of root, and so on. However,
when implementing, say, {\tt size}, the same sort of procedure follows in test-driven
development: test the null case, then the cases 1 node, 2 nodes, etc.

Once the data structure is implemented, this TDD process results in a large number
of redundant tests.

Grouping tests by data structure examples puts a much larger burden on TDD,
but provides a different perspective. These tests may be more effective once an
initial implementation is created. Then, pathological data structures can be
created and tested in detail.


A binary search tree using first 10 primes for keys:

%\ovalbox{%
  \begin{tikzpicture}[level/.style={sibling distance=50mm/#1}]
  \node [circle,draw] (z){$17$}
    child {node [circle,draw] (a) {$5$}
      child {node [circle,draw] (b) {$3$}
        child {node [circle, draw] (c) {$2$}}
        child {node (ca) {}}
      }
      child {node [circle,draw] (g) {$7$}
        child {node (ga) {}}
        child {node [circle,draw] (gb) {$11$}
          child {node {}}
          child {node [circle,draw] (gc) {$13$}}
        }
      }
    }
    child {node [circle,draw] (j) {$23$}
      child {node [circle,draw] (k) {$19$}}
    child {node [circle,draw] (l) {$29$}}
  };
\end{tikzpicture}
%}

This tree contains a pathological subtree rooted at the node with key 7.

\subsubsection{Insert}



\subsubsection{Collect}

Test driving may induce more tests than are strictly necessary.

\begin{enumerate}
\item Single node
\item 2 node tree right
\item 2 node tree left
\item arbitrary tree
\end{enumerate}

\subsection{Search}

Here is the algorithm to find a node in a binary search tree using the key:

\lstset{language={Python}}
\begin{lstlisting}[frame=single]

def search(self, key):
    if key == self.key:
        return self

    if key < self.key:
        if self.left is not None:
            self.left.search(key)
    else:
        if self.right is not None:
            self.right.search(key)
\end{lstlisting}

\subsubsection{Testing search}

Test cases:
\begin{enumerate}
\item finds nil with an empty tree
\item finds a node in a single node tree
\item finds the left child and right child in two node trees
\item find leaf node in tree with height greater than 1
\item finds nil when key is not in the tree
\end{enumerate}

\subsubsection{Delete}

Delete depends on search, so that will need to be implemented everywhere first.

There are three cases for the node to delete:

\begin{enumerate}
\item node has no children
\item node has one child
\item node has two children
\end{enumerate}

Other functions which are helpful for implementing delete correctly
are bst?, depth and size. These functions can all be used in the testing
code.

Write out a plan for implementing delete in java, python, c++ and lua,

\begin{itemize}
  \item including tree/node interaction, list of trees to work with,
  \item implementations for search, size, depth, and bst?
  \item algorithm writeup in bst doc, etc.
  \item After implementing and writeup, compare with clr.
\end{itemize}

Cormen et al.~\cite[p. 253]{cormen:th:1990} provide an implementation of
delete at the tree level. Advantages of this algorithm include explicit root node
deletion, and approximately retaining the tree's balance by using the deleted
node's successor.

However, it assumes that the node to delete is
given (rather than providing a key and serching for the node), and that
each node other than the root node has a link to its parent node.
Also, copying data such as the kay and whatever value(s) are associated
with the node is required. The number if {\tt if} statements also make
the logic difficult to see-at-a-glance.

\lstset{language={Python}}
\begin{lstlisting}[frame=single]
# Python implementation of delete, from CLR p. 253
def delete(T, z):
  if z.left is None or z.right is None:
      y = z
  else:
      y = z.successor(z)

  if y.left is not None:
      x = y.left
  else:
      x = y.right

  if x is not None:
      x.p = y.p # p is link to parent node

  if y.p is None:
      T.root = x
  else:
      if y == y.p.left:
          y.p.left = x
      else:
          y.p.right = x

  if y != z:
      z.key = y.key

  return y
\end{lstlisting}

Note that the returned node ({\tt y}) may or may not be the node
which was deleted. When the node to delete has two children, the
attributes of its successor node are copied into that node, and the
successor node is returned. While there is no impact at run time,
and copying attributes is arguably less complex than managing links
if moving a successor node, test cases must account for the discrepancy
in assertions. That is, removing a leaf node will return that leaf node,
for which an assertion may be written.

Here is an alternate delete algorithm in given in Ruby,
where the key provided instead of the node.
It does not require nodes to have parent links, the
parent is found along with the node. A disadvantage of
this scheme is dealing with a deleted root node. Any
enclosing tree container will have to update the
root node externally to this algorithm. Another, less
obvious disadvantage is this algorithm won't improve
the balance of the tree.

\lstset{language={Ruby}}
\begin{lstlisting}[frame=single]
def delete key, parent = nil
  node_to_delete, parent = search_with_parent key, parent
  left = node_to_delete.left
  right = node_to_delete.right

  if parent&.right == node_to_delete
    parent&.right = right
    right.insert left unless left.nil?
  else
    parent&.left = left
    left.insert right unless right.nil?
  end

  node_to_delete.left = node_to_delete.right = nil
  node_to_delete
end
\end{lstlisting}



Test cases:
\begin{enumerate}
\item deletes nil with an empty tree
\item deletes a node in a single node tree
\item deletes the left child and right child in two node trees
\item deletes leaf node in tree with height greater than 1
\item deletes nil when key is not in the tree
\item deletes the entire tree node-by-node
\item deleted node has key, left and right set to nil
\end{enumerate}

Yet another delete algorithm comes from CLRS~\cite{clrs}, with the following
cases:

\begin{enumerate}
  \item \textbf{Node z has no left child}: Replace z with it's right child
    which may or may not be nil.
  \item \textbf{Node z has left child but no right child}: Replace z with
    its left child.
  \item \textbf{Node z has 2 children where right child is successor}: 1.
    Replace z with successor; 2. Update sucessor left to point to z's left;
    3. Update z's left parent to point to z.
  \item \textbf{Node z has 2 children where right child is not successor}:
    1. Replace successor (y) with it's own right child; 2. Set y's right to
    z's right; 3. Set y's right parent to y; 4. Proceed with 3 above using
    z's right child successor.
\end{enumerate}


\subsubsection{AVL testing}

Some links for AVL trees:

\begin{itemize}
\item  \href{http://adtinfo.org/libavl.html/Step-3-in-AVL-Insertion.html}{%
http://adtinfo.org/libavl.html/Step-3-in-AVL-Insertion.html}
\item \href{http://eniac.cs.qc.cuny.edu/andrew/csci700-11/lecture7.pdf}{%
http://eniac.cs.qc.cuny.edu/andrew/csci700-11/lecture7.pdf}
\item \href{http://software.ucv.ro/~mburicea/lab6ASD.pdf}{%
http://software.ucv.ro/~mburicea/lab6ASD.pdf}
\end{itemize}

\paragraph{Insert}

\begin{itemize}
\item Draw out the cases requiring rotations. This will include 3, 4, 5, and 6
node examples, but probably no more than 6, possibly no more than 5.
\item Create a number of nodes.
\item Build the tree in sorted order starting with the smallest node.
This will build out the tree from the left, performing right, or ccw, rotations.
\item As each node is inserted, check the size, weight, balance and which node is currently the root of the tree.
\item Write an expectation for the rotate\_ccw method to ensure it's called the correct number of times.
\end{itemize}

Repeat this using the same list in decreasing order to build the tree from the
right.

Repeat inserting the keys in sorted order so the avl operation maintains
balance. Write an expectation to ensure no rotations are performed.



\paragraph{Delete}

Do something similar as above, check
rotations and balance after deletion.


\subsubsection{RB testing}

Some links for RB trees:

\begin{itemize}
\item \href{http://eniac.cs.qc.cuny.edu/andrew/csci700-11/lecture8.pdf}{%
http://eniac.cs.qc.cuny.edu/andrew/csci700-11/lecture8.pdf}
\end{itemize}

\subsection{Language listings}

\lstset{language=[Objective]{Caml}}
\begin{lstlisting}[frame=single]  % Start your code-block

open OUnit2;;

let test1 test_ctxt = assert_equal "x" (Foo.unity "x");;
let test2 test_ctxt = assert_equal 100 (Foo.unity 100);;

(* Name the test cases and group them together *)
let suite =
"suite">:::
 ["test1">:: test1;
  "test2">:: test2]
;;

let () =
  run_test_tt_main suite
;;
\end{lstlisting}

\section{Selected exercises and problems}


\subsection{CLRS, 3rd Edition}

\subsubsection{6.1-5} Is an array in sorted order a min-heap?

Yes, because for every index $i = 1, 2, 3,\ldots, A[i-1] \leq A[i]$,
which satisifies the min-heap property.

\subsubsection{12.1-1 Draw various height trees}

At some point in the future, I may actually draw these as for
display in this document. For now, here is the sequence
$[1, 4, 5, 10, 16, 17, 21]$ used to create binary search trees
of various heights. Each of the following sequences represents
a breadth-first listing, which corresponds to insertion order.

\begin{itemize}
  \item Height 2: $[10, 4, 17, 1, 5, 16, 21]$
  \item Height 3: $[5, 1, 17, 4, 16, 21, 10]$
  \item Height 4: $[5, 1, 10, 4, 16, 17, 21]$
  \item Height 5: $[4, 1, 5, 10, 16, 17, 21]$
  \item Height 6: $[1, 4, 5, 10, 16, 17, 21]$
\end{itemize}

\subsubsection{12.1-2 Compare binary-search-tree and min-heap}

What is the difference between the binary-search-tree property and the min-heap
property (p. 153)? Can the min-heap property be used to print out the keys of
an $n$-node tree, in sorted order, in $O(n)$ time?  Show how, or explain why
not.

Let's start with definitions.
The min-heap property:

\begin{equation}
  A[\mathrm{parent}] \leq A[i].
\end{equation}

The binary-search-tree property: For $x$ a node in a in binary search tree,
$y$ in the left subtree implies $y.\mathrm{key} \leq x.\mathrm{key}$, and
$y$ in the right subtree implies $y.\mathrm{key} \geq x.\mathrm{key}$.

The difference is the min-heap requires \emph{both} children to be
greater than the root node, violating the binary-search-tree property.

In-order printing the keys of an arbitrary min-heap cannot be done in $O(n)$
for either breadth-first, or any depth-first traversal without inducing
additional comparisons at each level. Consider the min-heap $[2, 8, 4]$.
Breadth-first traversal requires comparing elements 2 and 3.

There may be a result elsewhere proving sorting requires $O(n\ln n)$,
this would be a good place to cite that result if the conditions hold.

\subsubsection{12.1-3 Iterative algorithm for in-order tree walk}

\subsubsection{12.1-4 Derive recursive pre- and post-order walks}

Implementing pre- and post-order-traverse in C is not difficult. Getting
such traverses to do work requires passing in an argument to handle
the accumulated state, and a function with which to compute that state.
This would be handled by closures in languages which support closures.
Passing in both function pointer and data allows the passed in function
to properly type and dereference the \texttt{userdata}.

\lstset{language={c}}
\begin{lstlisting}[frame=single,title=Post-order traverse]
void
post_order_traverse(Node * n, Callback callback, void * userdata) {
  if (n->left  != NULL) { post_order_traverse(n->left, callback, userdata); }
  if (n->right != NULL) { post_order_traverse(n->right, callback, userdata); }
  if (callback != NULL) { callback(n, userdata); }
}
\end{lstlisting}

\lstset{language={c}}
\begin{lstlisting}[frame=single,title=Pre-order traverse]
void
pre_order_traverse(Node * n, Callback callback, void * userdata) {
  if (callback != NULL) { callback(n, userdata); }
  if (n->left  != NULL) { in_order_traverse(n->left, callback, userdata); }
  if (n->right != NULL) { in_order_traverse(n->right, callback, userdata); }
}
\end{lstlisting}


\subsubsection{12.1-5 Worst case comparisons on sorted data}

\subsubsection{12.2-1 Valid search sequence}

\begin{quote}
Suppose that we have numbers between 1 and 1000 in a binary search tree, and we
want to search for the number 363. Which of the following sequences could not be
the sequence of nodes examined?

a. 2, 252, 401, 398, 330, 344, 397, 363.

b. 924, 220, 911, 244, 898, 258, 362, 363.

c. \textbf{INVALID} 925, 202, 911, 240, 912, 245, 363.

d. 2, 399, 387, 219, 266, 382, 381, 278, 363.

e. \textbf{INVALID} 935, 278, 347, 621, 299, 392, 358, 363.
\end{quote}

Search and insertion are conceptually similar in binary search
trees. To determine validity, each sequence above was drawn as
a binary search tree. After drawing, it's obvious which sequences
are not valid for finding the desired node.

\subsubsection{12.2-2 Iterative implementations of min and max}

The original problem calls for recursive implementations for minimum
and maximum elements, but I already derived those, so this exercise
is changed to provide the iterative implementations. Here's the
Ruby code for \texttt{minimum}:

\lstset{language={Ruby}}
\begin{lstlisting}[frame=single]
def minimum
  min = self.left
  while min.left
    min = min.left
  end
  min
end
\end{lstlisting}

The \texttt{maximum} function works by successively calling the right
child, as one might suspect.

\subsubsection{12.2-3 Tree predecessor}

This is already written, recursively, without using parent pointer.


\subsubsection{12.2-4 Prof. Bunyan's key sorting claim}

\subsubsection{12.2-5 Successor and predecessor for node with 2 children}

\subsubsection{12.2-6 Ancestor nodes}

\subsubsection{12.2-7 In-order walk using minimum and successor}

\subsubsection{12.2-8 Complexity of successive calls to successor}

\subsubsection{12.2-9 Node parent-child key relationship}


\subsubsection{12.4-4 Prove $2^x$ is convex}

Proving $2^x$ is convex is a bit tricky, and leans on a fairly deep result about
exponentiation which I don't really understand yet. We can start with a definition
of convexity, with the assumption $x\in\mathbb{R}$:

\begin{equation}
  f''(x) \geq 0.
\end{equation}

Differentiating $f(x) = 2^z$ twice,

\begin{equation}
  f' = 2^x \ln 2, f'' = 2^x\ln^2 2.
\end{equation}

Showing the second derivative is positive requires showing that the
terms in the product are both positive, or both negative. Since $\ln 2$
is positive, we must show that $2^x$ for all $x$.

By definition,

\begin{equation}\label{eq:exp_series}
  e^x = \sum_{n=0}^\infty \frac{x^n}{n!}.
\end{equation}

Also by definition,

\begin{equation}\label{eq:log2}
  2^x = e^{x\ln 2}
\end{equation}

Using the result $e^xe^y = e^{(x+y)}$, we note that $e^xe^{-x} = 1$, implying

\begin{equation}\label{eq:invert}
  e^{-x} = \frac{1}{e^x}.
\end{equation}

For $x > 0$, every term in the power series expansion
(Eq.~\ref{eq:exp_series}) for $e^x$ is positive.
From Eq.~\ref{eq:invert}, $e^x$ and $e^{-x}$ are positive for all $x\in\mathbb{R}$. The result
$2^x > 0$ for all $x\in\mathbb{R}$ follows by substitution using
Eq.~\ref{eq:log2}. Hence $2^x\ln^2 2 > 0$ and $2^x$ is convex.

\begin{comment}
Substituting and expanding:

\begin{equation}
  2^x = 1 + x\ln 2 + \frac{x^2 \ln^2 2}{2!} + \frac{x^3 \ln^3 2}{3!} + \cdots.
\end{equation}

For $x > 0, 2^x$ is clearly positive. For $x = 0, 2^0 = 1$, again positive. When $x < 0$,

\begin{equation}
  2^{-x} = 1 - x\ln 2 + \frac{x^2 \ln^2 2}{2!} - \frac{x^3 \ln^3 2}{3!} + \frac{x^4 \ln^4 2}{4!}
  - \frac{x^5 \ln^5 2}{5!} + \cdots.
\end{equation}
\end{comment}


\subsubsection{Problem 12-1}

\subsubsection{Problem 12-2}

\subsubsection{Problem 12-3}

\subsubsection{Problem 12-4: Number of different binary trees}

The goal of this problem is to determine the number of binary
trees possible given $n$ nodes.

\paragraph{Part a} For $b_0 = 1$ and $n \geq 1$,

\[
b_n = \sum_{k=0}^{n-1} b_kb_{n-1-k}.
\]

Let's do some examples, starting with $n = 1$, giving $k = 0$:

\begin{itemize}
\item [$k=0$] $b_0 = 1, b_1$
\end{itemize}

\paragraph{Part b.} Let $B(x)$ be the generating function

\begin{equation}
B(x) = \sum_{n=0}^{\infty}b_nx^n.
\end{equation}

Show that $B(x) = xB(x)^2 + 1$, and

\begin{equation}
B(x) = \frac{1}{2x}(1-\sqrt{1-4x}).
\end{equation}


\subsection{Sedgewick and Wayne, 4th Edition}

Sedgewick has been publishing an algorithms book for a very long time.
This latest version with Wayne seems pretty good. They take a slightly
different approach than Cormen et al., and it's worth working through
a number of their exercises as well.

\subsubsection{3.2.1 E A S Y Q U E S T I O N}



\subsubsection{3.2.4 Invalid search sequences}

\begin{quote}
Suppose that a certain BST has keys that are integers between 1 and 10, and we
search for 5. Which sequence below cannot be the sequence of keys examined?

a. 10, 9, 8, 7, 6, 5

b. 4, 10, 8, 7, 53

c. 1, 10, 2, 9, 3, 8, 4, 7, 6, 5

d. 2, 7, 3, 8, 4, 5

e. 1, 2, 10, 4, 8, 5
\end{quote}

\section{Detours}

Detours fill in a bit of mathematical background material.
First, a short detour into generating functions.

\subsection{Detour into generating functions}

This example is taken from Cormen et al.~\cite[p. 74]{cormen:th:1990}
Problem 4-6.

\begin{align}
F(z) & = \sum_{i=0}^{\infty} F_ix^i\\
\label{eqn:fibonacci_opsgf}
     & = 0 + z + z^2 + 2z^3 + 3z^4 + 5z^5 + 8z^6 + 13z^7 + 21z^8+\cdots
\end{align}

This is inconvenient for a closed form solution, so we need to do a bit of
manipulation. First, multiply both sides by $Z$:

\begin{equation}
\label{eqn:zfibonacci_opsgf}
zF(z) = z^2 + z^3 + 2z^4 + 3z^5 + 5z^6 + 8z^7 + 13z^8 + 21z^9+\cdots
\end{equation}

Now subtract~\ref{eqn:zfibonacci_opsgf} from~\ref{eqn:fibonacci_opsgf} to
obtain

\begin{align}
F(z) - zF(z) & = z + z^3 + z^4 + 2z^5 + 3z^6 +\cdots\\
F(z) - zF(z) & = z + z^2(0 + z + z^2 + 2Z^3 + 3z^4 +\cdots)\\
             & = z + z^2F(z)\\
        F(z) & = \frac{z}{1-z-z^2}.
\end{align}


\subsection{Detour into partial fraction expansion}

The idea of partial fraction expansion is to reduce some
complicated ratio of functions into a sum of simpler functions.
Practical uses include analytic solutions for differential equations
using the Laplace Transform~\cite[p. 347]{nagle:rk1989}, and simplifying
generating functions, as will done for deriving a formula for the
$nth$ Catalan number.

Here's a simple example:

\begin{equation}
\frac{-4x-10}{x^2+4x+3} = \frac{A}{x+1} + \frac{B}{x+3}
\end{equation}

Solving for $A$ and $B$ can be done using a system of linear equations,
or, in this case, simply assuming the values $x = -1$ and $x = -5$
to solve for $A$ and $B$ respectively. Either technique results in $A = -3$ and
$B = -1$:
\begin{equation}
\frac{-4x-10}{x^2+4x+3} = \frac{-3}{x+1} + \frac{-1}{x+3},
\end{equation}
which can of course be verified by summing the right hand side.


\subsection{Detour into Taylor series}

Cormen et al.~\cite[p. 262]{cormen:th:1990} gives Taylor Series definition as
(there is an error in the edition cited here, see the Errata for clarification):

\begin{equation}
f(x) = \sum_{k=0}^{\infty} \frac{f^{(k)}(a)}{k!}(x-a)^k,
\end{equation}

and suggests expanding the radical term in the generation function
$\sqrt{1-4x}$ around $0$.

{\bf TODO: write out a few derivatives of the radical to construct
the Taylor Series.}

\section{Tikz}

% \href{http://www.texample.net/tikz/examples/merge-sort-recursion-tree/}{From texample.net}

\ovalbox{%
  \begin{tikzpicture}[level/.style={sibling distance=50mm/#1}]
  \node [circle,draw] (z){$17$}
    child {node [circle,draw] (a) {$5$}
      child {node [circle,draw] (b) {$3$}
        child {node [circle, draw] (c) {$2$}}
        child {node (ca) {}}
      }
      child {node [circle,draw] (g) {$7$}
        child {node (ga) {}}
        child {node [circle,draw] (gb) {$11$}
          child {node {}}
          child {node [circle,draw] (gc) {$13$}}
        }
      }
    }
    child {node [circle,draw] (j) {$23$}
      child {node [circle,draw] (k) {$19$}}
    child {node [circle,draw] (l) {$29$}}
  };
\end{tikzpicture}
}



\href{http://www.texample.net/tikz/examples/merge-sort-recursion-tree/}{From texample.net}

\ovalbox{%
%\begin{tikzpicture}[level/.style={sibling distance=60mm/#1}]
\begin{tikzpicture}[level/.style={sibling distance=40mm/#1}]
\node [circle,draw] (z){$n$}
  child {node [circle,draw] (a) {$\frac{n}{2}$}
    child {node [circle,draw] (b) {$\frac{n}{2^2}$}
      child {node {$\vdots$}
        child {node [circle,draw] (d) {$\frac{n}{2^k}$}}
        child {node [circle,draw] (e) {$\frac{n}{2^k}$}}
      }
      child {node {$\vdots$}}
    }
    child {node [circle,draw] (g) {$\frac{n}{2^2}$}
      child {node {$\vdots$}}
      child {node {$\vdots$}}
    }
  }
  child {node [circle,draw] (j) {$\frac{n}{2}$}
    child {node [circle,draw] (k) {$\frac{n}{2^2}$}
      child {node {$\vdots$}}
      child {node {$\vdots$}}
    }
  child {node [circle,draw] (l) {$\frac{n}{2^2}$}
    child {node {$\vdots$}}
    child {node (c){$\vdots$}
      child {node [circle,draw] (o) {$\frac{n}{2^k}$}}
      child {node [circle,draw] (p) {$\frac{n}{2^k}$}
        child [grow=right] {node (q) {$=$} edge from parent[draw=none]
          child [grow=right] {node (q) {$O_{k = \lg n}(n)$} edge from parent[draw=none]
            child [grow=up] {node (r) {$\vdots$} edge from parent[draw=none]
              child [grow=up] {node (s) {$O_2(n)$} edge from parent[draw=none]
                child [grow=up] {node (t) {$O_1(n)$} edge from parent[draw=none]
                  child [grow=up] {node (u) {$O_0(n)$} edge from parent[draw=none]}
                }
              }
            }
            child [grow=down] {node (v) {$O(n \cdot \lg n)$}edge from parent[draw=none]}
          }
        }
      }
    }
  }
};
\path (a) -- (j) node [midway] {+};
\path (b) -- (g) node [midway] {+};
\path (k) -- (l) node [midway] {+};
\path (k) -- (g) node [midway] {+};
\path (d) -- (e) node [midway] {+};
\path (o) -- (p) node [midway] {+};
\path (o) -- (e) node (x) [midway] {$\cdots$}
  child [grow=down] {
    node (y) {$O\left(\displaystyle\sum_{i = 0}^k 2^i \cdot \frac{n}{2^i}\right)$}
    edge from parent[draw=none]
  };
\path (q) -- (r) node [midway] {+};
\path (s) -- (r) node [midway] {+};
\path (s) -- (t) node [midway] {+};
\path (s) -- (l) node [midway] {=};
\path (t) -- (u) node [midway] {+};
\path (z) -- (u) node [midway] {=};
\path (j) -- (t) node [midway] {=};
\path (y) -- (x) node [midway] {$\Downarrow$};
\path (v) -- (y)
  node (w) [midway] {$O\left(\displaystyle\sum_{i = 0}^k n\right) = O(k \cdot n)$};
\path (q) -- (v) node [midway] {=};
\path (e) -- (x) node [midway] {+};
\path (o) -- (x) node [midway] {+};
\path (y) -- (w) node [midway] {$=$};
\path (v) -- (w) node [midway] {$\Leftrightarrow$};
\path (r) -- (c) node [midway] {$\cdots$};
\end{tikzpicture}}


\end{document}
